{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# EECS 491 Final Project: Bayesian Structure and Parameter Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "Bayesian-network models are a commonly-used framework for modeling conditional probabilities, allowing extremely complicated conditional relationships to be modeled and implemented with ease. Previously in this course, we have covered in great detail how to perform inference operations given a defined bayesian-network model. This works well when we are able to construct an informed model, which includes the correct conditional probabilities and links. However, what are we to do when we are given lots of data, with no knowledge of the relationships between any of the variables or elements? Before we can perform any kind of predictive inference, we first need to learn a bayesian model from our data. This can generally be described in two parts: learning the structural relationship between probabilistic variables, and learning the conditional probability distributions which fit data to a given structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Graph, Digraph\n",
    "import pgmpy\n",
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.factors.discrete import TabularCPD\n",
    "from pgmpy.inference import VariableElimination\n",
    "from pgmpy.sampling import GibbsSampling\n",
    "from pgmpy.estimators import ExhaustiveSearch, HillClimbSearch, ConstraintBasedEstimator\n",
    "from pgmpy.estimators import BDeuScore, K2Score, BicScore\n",
    "from pgmpy.estimators import MaximumLikelihoodEstimator, BayesianEstimator\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## True Model\n",
    "\n",
    "In order to generate bayesian-networks from scratch, we need data to describe. Throughout this example, we will be using the following model (Adpted from Barber Exercise 3.6) that describes the probability of a car starting given certain factors. We will use this model to generate samples which will be used to generate networks and parameters, and then we will compare the accuracy of the generated models to the actual model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"220pt\" height=\"188pt\"\n",
       " viewBox=\"0.00 0.00 220.35 188.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 184)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-184 216.3454,-184 216.3454,4 -4,4\"/>\n",
       "<!-- b -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>b</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"93.3454\" cy=\"-162\" rx=\"36.2938\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"93.3454\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Battery</text>\n",
       "</g>\n",
       "<!-- g -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>g</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"154.3454\" cy=\"-90\" rx=\"33.5952\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"154.3454\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Gauge</text>\n",
       "</g>\n",
       "<!-- b&#45;&gt;g -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>b&#45;&gt;g</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M107.4934,-145.3008C115.2694,-136.1226 125.0586,-124.568 133.6757,-114.3971\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"136.4805,-116.501 140.2742,-106.6087 131.1396,-111.9761 136.4805,-116.501\"/>\n",
       "</g>\n",
       "<!-- t -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>t</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"51.3454\" cy=\"-90\" rx=\"51.1914\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"51.3454\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Turns Over</text>\n",
       "</g>\n",
       "<!-- b&#45;&gt;t -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>b&#45;&gt;t</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M83.1784,-144.5708C78.2987,-136.2055 72.3443,-125.998 66.911,-116.6839\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"69.7726,-114.6431 61.7106,-107.7689 63.7262,-118.1702 69.7726,-114.6431\"/>\n",
       "</g>\n",
       "<!-- f -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>f</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"185.3454\" cy=\"-162\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"185.3454\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Fuel</text>\n",
       "</g>\n",
       "<!-- f&#45;&gt;g -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>f&#45;&gt;g</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M177.8412,-144.5708C174.277,-136.2927 169.936,-126.2104 165.9598,-116.9752\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"169.1653,-115.5696 161.9959,-107.7689 162.7359,-118.3379 169.1653,-115.5696\"/>\n",
       "</g>\n",
       "<!-- s -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>s</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"133.3454\" cy=\"-18\" rx=\"30.5947\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"133.3454\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Starts</text>\n",
       "</g>\n",
       "<!-- f&#45;&gt;s -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>f&#45;&gt;s</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M193.1016,-144.5674C200.2504,-125.7868 208.239,-95.4445 197.3454,-72 190.4767,-57.2174 177.3474,-45.025 164.7594,-35.9896\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"166.4665,-32.9199 156.2147,-30.2457 162.5613,-38.7293 166.4665,-32.9199\"/>\n",
       "</g>\n",
       "<!-- t&#45;&gt;s -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>t&#45;&gt;s</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M70.7783,-72.937C82.1226,-62.9762 96.5464,-50.3114 108.6854,-39.6527\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"111.0721,-42.2149 116.2772,-32.9868 106.4534,-36.9548 111.0721,-42.2149\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f34dc0161d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q5 = Digraph()\n",
    "q5.node('b', 'Battery')\n",
    "q5.node('g', 'Gauge')\n",
    "q5.node('f', 'Fuel')\n",
    "q5.node('t', 'Turns Over')\n",
    "q5.node('s', 'Starts')\n",
    "q5.edges(['bg','fg','bt','ts','fs'])\n",
    "q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is valid:  True\n",
      "True Model Edges:  [('B', 'G'), ('B', 'T'), ('F', 'G'), ('F', 'S'), ('T', 'S')]\n"
     ]
    }
   ],
   "source": [
    "# Create model from list of edges\n",
    "model = BayesianModel([('B','G'),('F','G'),('B','T'),('T','S'),('F','S')])\n",
    "\n",
    "# define p(B) and p(M) \n",
    "# variable_card is cardinality = 2 for true|false\n",
    "# values are defined in numeric order p(x_i = [false, true]), ie  [0, 1]\n",
    "priorB = TabularCPD(variable='B', variable_card=2, values=[[0.02, 0.98]])\n",
    "priorF = TabularCPD(variable='F', variable_card=2, values=[[0.05, 0.95]])\n",
    "\n",
    "# define p(G|B,F)\n",
    "# Variables cycle in numerical order of evidence values,\n",
    "# ie BF = 00, 01, 10, 11 for each value of G.\n",
    "cpdG = TabularCPD(variable='G', variable_card=2, \n",
    "                  evidence=['B', 'F'], evidence_card=[2, 2],\n",
    "                  values=[[0.99, 0.1, 0.97, 0.04], \n",
    "                          [0.01, 0.9, 0.03, 0.96]])\n",
    "\n",
    "# define p(T|B)\n",
    "cpdT = TabularCPD(variable='T', variable_card=2,\n",
    "                 evidence=['B'], evidence_card=[2],\n",
    "                 values=[[0.98, 0.03],\n",
    "                         [0.02, 0.97]])\n",
    "\n",
    "# define p(S|T,F)\n",
    "cpdS = TabularCPD(variable='S', variable_card=2, \n",
    "                  evidence=['F','T'], evidence_card=[2, 2],\n",
    "                  values=[[0.99, 0.92, 1.0, 0.01], \n",
    "                          [0.01, 0.08, 0.0, 0.99]])\n",
    "\n",
    "# add probabilities to model\n",
    "model.add_cpds(priorB, priorF, cpdG, cpdT, cpdS)\n",
    "print('Model is valid: ', model.check_model())\n",
    "print('True Model Edges: ',model.edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a defined model, let's generate some data using Gibbs sampling. Different amounts of samples are collected, in order to test the number of required samples for each of the later-implemented algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 1132.47it/s]\n",
      "100%|██████████| 99/99 [00:00<00:00, 1471.87it/s]\n",
      "100%|██████████| 999/999 [00:00<00:00, 1577.62it/s]\n",
      "100%|██████████| 9999/9999 [00:04<00:00, 2380.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# generate sets of samples\n",
    "sampler = GibbsSampling(model)\n",
    "sample10 = sampler.sample(size=10, return_type='dataframe')\n",
    "sample100 = sampler.sample(size=100, return_type='dataframe')\n",
    "sample1000 = sampler.sample(size=1000, return_type='dataframe')\n",
    "sample10000 = sampler.sample(size=10000, return_type='dataframe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Learning\n",
    "\n",
    "In this section, we will focus on the problem of fitting a given bayesian structure to real data. Specifically, this requires finding the conditional probability distributions (CPDs) for each variable such that the joint probability distribution $p(x)$ accurately describes the observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimation\n",
    "\n",
    "We can assume that in a bayesian network, the structure of $p(x)$ can be constrained as\n",
    "\n",
    "$$p(x) = \\prod_{i=1}^{K}p(x_i|pa(x_i)),$$\n",
    "\n",
    "where $K$ is the number of variables, $x_i$ is a given variable and $pa(x_i)$ are the parents of $x_i$ in the directed acyclic graph.\n",
    "\n",
    "One method of extimating the CPDs for each variable is to determine the maximum likelihood of each term. To do this, we can minimize the KL divergence between the data distribution $q(x)$ and $p(x)$. The setting which minimizes this divergence and maximizes the overall likelihood is\n",
    "\n",
    "$$p(x_i|pa(x_i)) = q(x_i|pa(x_i),$$\n",
    "\n",
    "which can be written in terms of the given data,\n",
    "\n",
    "$$p(x_i=s|pa(x_i)=t) \\propto \\sum_{n=1}^{N}\\mathbb{I}[x_i^n=s,pa(x_i^n)=t],$$\n",
    "\n",
    "saying that each CPD table entry can be set by using the relative frequencies of each state, meaning that each entry is the ratio of the number of times the state $\\{x_i=s,pa(x_i)=t\\}$ occurs vs the sum of all other states $\\{x_i=s',pa(x_i)=t\\}$. This operation is fairly computationally efficient, as it just requires counting.\n",
    "\n",
    "Below, we fit CPDs to our example problem given the correct structure, and compare our results to the true values using 10,000 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Starts CPD:\n",
      " +------+------+------+------+------+\n",
      "| F    | F(0) | F(0) | F(1) | F(1) |\n",
      "+------+------+------+------+------+\n",
      "| T    | T(0) | T(1) | T(0) | T(1) |\n",
      "+------+------+------+------+------+\n",
      "| S(0) | 0.99 | 0.92 | 1.0  | 0.01 |\n",
      "+------+------+------+------+------+\n",
      "| S(1) | 0.01 | 0.08 | 0.0  | 0.99 |\n",
      "+------+------+------+------+------+\n",
      "MLE Starts CPD:\n",
      " +------+------+---------------------+------+----------------------+\n",
      "| F    | F(0) | F(0)                | F(1) | F(1)                 |\n",
      "+------+------+---------------------+------+----------------------+\n",
      "| T    | T(0) | T(1)                | T(0) | T(1)                 |\n",
      "+------+------+---------------------+------+----------------------+\n",
      "| S(0) | 1.0  | 0.9113924050632911  | 1.0  | 0.008637655805816751 |\n",
      "+------+------+---------------------+------+----------------------+\n",
      "| S(1) | 0.0  | 0.08860759493670886 | 0.0  | 0.9913623441941832   |\n",
      "+------+------+---------------------+------+----------------------+\n"
     ]
    }
   ],
   "source": [
    "mle_model = MaximumLikelihoodEstimator(model, sample10000)\n",
    "mle_cpds = mle_model.get_parameters()\n",
    "\n",
    "print('True Starts CPD:\\n',cpdS)\n",
    "print('MLE Starts CPD:\\n',mle_cpds[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One CPD is displayed, for the sake of brevity. As we can see, the MLE method is able to approximate each of the tabld entries within an error of about 1%. Note however that MLE overfits to the data, as seen when the relationship in the first column is lost, as following the MLE results the car should never start in that condition. While that is acceptable in this problem, it may not be in others.\n",
    "\n",
    "As we can see below, the accuracy of our model fit is dependent on the number and diversity of samples we receive. As the amount of available data decreases, our error in each entry also increases, and the overall CPD has less possibilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Starts CPD:\n",
      " +------+------+------+------+------+\n",
      "| F    | F(0) | F(0) | F(1) | F(1) |\n",
      "+------+------+------+------+------+\n",
      "| T    | T(0) | T(1) | T(0) | T(1) |\n",
      "+------+------+------+------+------+\n",
      "| S(0) | 0.99 | 0.92 | 1.0  | 0.01 |\n",
      "+------+------+------+------+------+\n",
      "| S(1) | 0.01 | 0.08 | 0.0  | 0.99 |\n",
      "+------+------+------+------+------+\n",
      "1000 Samples:\n",
      " +------+------+---------------------+------+---------------------+\n",
      "| F    | F(0) | F(0)                | F(1) | F(1)                |\n",
      "+------+------+---------------------+------+---------------------+\n",
      "| T    | T(0) | T(1)                | T(0) | T(1)                |\n",
      "+------+------+---------------------+------+---------------------+\n",
      "| S(0) | 1.0  | 0.9478260869565217  | 1.0  | 0.01264367816091954 |\n",
      "+------+------+---------------------+------+---------------------+\n",
      "| S(1) | 0.0  | 0.05217391304347826 | 0.0  | 0.9873563218390805  |\n",
      "+------+------+---------------------+------+---------------------+\n",
      "100 Samples:\n",
      " +------+---------------------+----------------------+\n",
      "| F    | F(0)                | F(1)                 |\n",
      "+------+---------------------+----------------------+\n",
      "| T    | T(1)                | T(1)                 |\n",
      "+------+---------------------+----------------------+\n",
      "| S(0) | 0.8333333333333334  | 0.010638297872340425 |\n",
      "+------+---------------------+----------------------+\n",
      "| S(1) | 0.16666666666666666 | 0.9893617021276596   |\n",
      "+------+---------------------+----------------------+\n",
      "10 Samples:\n",
      " +------+------+--------------------+\n",
      "| F    | F(1) | F(1)               |\n",
      "+------+------+--------------------+\n",
      "| T    | T(0) | T(1)               |\n",
      "+------+------+--------------------+\n",
      "| S(0) | 1.0  | 0.1111111111111111 |\n",
      "+------+------+--------------------+\n",
      "| S(1) | 0.0  | 0.8888888888888888 |\n",
      "+------+------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "mle_model1000 = MaximumLikelihoodEstimator(model, sample1000)\n",
    "mle_cpds1000 = mle_model1000.get_parameters()\n",
    "mle_model100 = MaximumLikelihoodEstimator(model, sample100)\n",
    "mle_cpds100 = mle_model100.get_parameters()\n",
    "mle_model10 = MaximumLikelihoodEstimator(model, sample10)\n",
    "mle_cpds10 = mle_model10.get_parameters()\n",
    "\n",
    "print('True Starts CPD:\\n',cpdS)\n",
    "print('1000 Samples:\\n',mle_cpds1000[3])\n",
    "print('100 Samples:\\n',mle_cpds100[3])\n",
    "print('10 Samples:\\n',mle_cpds10[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Estimation\n",
    "\n",
    "One alternative method which we can use to try and avoid overfitting to the data is a bayesian approach, where we maintain a distribution over the parameters. In order to do this we will make some assumptions, the first being that the parameters are globally independent,\n",
    "\n",
    "$$p(\\theta_1,\\theta_2,...,\\theta_n) = p(\\theta_1)p(\\theta_2)...p(\\theta_n).$$\n",
    "\n",
    "The second assumption we'll make is that the local parameters are also independent,\n",
    "\n",
    "$$p(\\theta_i) = p(\\theta_i^{0,0,...,0})p(\\theta_i^{0,1,...,0})...p(\\theta_i^{1,1,...,1}).$$\n",
    "\n",
    "We'll use a general multivariate model, even though our example only consists of binary states. For our conjugate prior, we'll use the Dirichlet distribution. Since our posterior factorizes over the variables due to the global independence assumption, each posterior table for a variable depends only on the data $D(v)$ of the variable's family. With the Dirichlet prior\n",
    "\n",
    "$$p(\\theta(v;j)) = Dirichlet(\\theta(v,j)|u(v;j))$$\n",
    "\n",
    "where $u$ is the Dirichlet hyperparameters, the posterior is also a Dirichlet\n",
    "\n",
    "$$p(\\theta(v)|D(v)) = \\prod_jDirichlet(\\theta(v;j)|u'(v;j)),$$\n",
    "\n",
    "where $u'$ is the updated hyperparameter prior, given by\n",
    "\n",
    "$$u_i'(v;j) = u_i(v;j)+\\#(v=i,pa(v)=j)$$\n",
    "\n",
    "with $\\#$ denoting the number of counts in the data for that state. This leads to a likelihood (or *score*) which we can iteratively maximize, given by\n",
    "\n",
    "$$p(D) = \\prod_k\\prod_np(v_k^n|pa(v_k^n)) = \\prod_k\\prod_j\\frac{Z(u'(v_k;j))}{Z(u(v_k;j))},$$\n",
    "\n",
    "where $Z(u)$ is the normalization constant of a Dirichlet distribution with hyperparameters $u$.\n",
    "\n",
    "There are a couple of methods for initializing the Dirichlet prior hyperparameters $u$. The first, and most straightforward, way is to set all of the values $u_i(v;j)$ to 1. This is refered to as the *K2* prior. Another method is known as the *BDeu* (Bayesian Dirichlet equivalent uniform prior), where the hyperparameters are set by\n",
    "\n",
    "$$u_i(v;j) = \\frac{\\alpha}{dim(v)dim(pa(v))},$$\n",
    "\n",
    "with an equivalent sample size $\\alpha$. Varying this parameter influences the performance of the estimation. We compare the performance of these two different priors below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Starts CPD:\n",
      " +------+------+------+------+------+\n",
      "| F    | F(0) | F(0) | F(1) | F(1) |\n",
      "+------+------+------+------+------+\n",
      "| T    | T(0) | T(1) | T(0) | T(1) |\n",
      "+------+------+------+------+------+\n",
      "| S(0) | 0.99 | 0.92 | 1.0  | 0.01 |\n",
      "+------+------+------+------+------+\n",
      "| S(1) | 0.01 | 0.08 | 0.0  | 0.99 |\n",
      "+------+------+------+------+------+\n",
      "K2 Starts CPD:\n",
      " +------+------+---------------------+---------------------+----------------------+\n",
      "| F    | F(0) | F(0)                | F(1)                | F(1)                 |\n",
      "+------+------+---------------------+---------------------+----------------------+\n",
      "| T    | T(0) | T(1)                | T(0)                | T(1)                 |\n",
      "+------+------+---------------------+---------------------+----------------------+\n",
      "| S(0) | 0.8  | 0.9401709401709402  | 0.9285714285714286  | 0.013761467889908258 |\n",
      "+------+------+---------------------+---------------------+----------------------+\n",
      "| S(1) | 0.2  | 0.05982905982905983 | 0.07142857142857142 | 0.9862385321100917   |\n",
      "+------+------+---------------------+---------------------+----------------------+\n",
      "BDeu Starts CPD:\n",
      " +------+---------------------+---------------------+---------------------+----------------------+\n",
      "| F    | F(0)                | F(0)                | F(1)                | F(1)                 |\n",
      "+------+---------------------+---------------------+---------------------+----------------------+\n",
      "| T    | T(0)                | T(1)                | T(0)                | T(1)                 |\n",
      "+------+---------------------+---------------------+---------------------+----------------------+\n",
      "| S(0) | 0.8529411764705882  | 0.943010752688172   | 0.9528301886792453  | 0.013342898134863702 |\n",
      "+------+---------------------+---------------------+---------------------+----------------------+\n",
      "| S(1) | 0.14705882352941177 | 0.05698924731182796 | 0.04716981132075472 | 0.9866571018651363   |\n",
      "+------+---------------------+---------------------+---------------------+----------------------+\n"
     ]
    }
   ],
   "source": [
    "bayesEst = BayesianEstimator(model, sample1000)\n",
    "k2_cpds = bayesEst.get_parameters(prior_type='K2')\n",
    "bd_cpds = bayesEst.get_parameters(prior_type='BDeu',equivalent_sample_size=5)\n",
    "\n",
    "print('True Starts CPD:\\n',cpdS)\n",
    "print('K2 Starts CPD:\\n',k2_cpds[4])\n",
    "print('BDeu Starts CPD:\\n',bd_cpds[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above that both choices of prior lead to a more conservative estimation of the CPDs than MLE, with the BDeu prior having more accuracy than the K2 prior. We see more error than MLE (13% error vs 1%), however the relationship is still preserved. Varying the $\\alpha$ parameter for the BDeu prior leads to changes in performance, shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Starts CPD:\n",
      " +------+------+------+------+------+\n",
      "| F    | F(0) | F(0) | F(1) | F(1) |\n",
      "+------+------+------+------+------+\n",
      "| T    | T(0) | T(1) | T(0) | T(1) |\n",
      "+------+------+------+------+------+\n",
      "| S(0) | 0.99 | 0.92 | 1.0  | 0.01 |\n",
      "+------+------+------+------+------+\n",
      "| S(1) | 0.01 | 0.08 | 0.0  | 0.99 |\n",
      "+------+------+------+------+------+\n",
      "alpha = 1 Starts CPD:\n",
      " +------+----------------------+---------------------+---------------------+----------------------+\n",
      "| F    | F(0)                 | F(0)                | F(1)                | F(1)                 |\n",
      "+------+----------------------+---------------------+---------------------+----------------------+\n",
      "| T    | T(0)                 | T(1)                | T(0)                | T(1)                 |\n",
      "+------+----------------------+---------------------+---------------------+----------------------+\n",
      "| S(0) | 0.9615384615384616   | 0.9468546637744034  | 0.9897959183673469  | 0.012783682849755817 |\n",
      "+------+----------------------+---------------------+---------------------+----------------------+\n",
      "| S(1) | 0.038461538461538464 | 0.05314533622559653 | 0.01020408163265306 | 0.9872163171502442   |\n",
      "+------+----------------------+---------------------+---------------------+----------------------+\n",
      "alpha = 10 Starts CPD:\n",
      " +------+---------------------+---------------------+---------------------+----------------------+\n",
      "| F    | F(0)                | F(0)                | F(1)                | F(1)                 |\n",
      "+------+---------------------+---------------------+---------------------+----------------------+\n",
      "| T    | T(0)                | T(1)                | T(0)                | T(1)                 |\n",
      "+------+---------------------+---------------------+---------------------+----------------------+\n",
      "| S(0) | 0.7727272727272727  | 0.9382978723404255  | 0.9137931034482759  | 0.014040114613180516 |\n",
      "+------+---------------------+---------------------+---------------------+----------------------+\n",
      "| S(1) | 0.22727272727272727 | 0.06170212765957447 | 0.08620689655172414 | 0.9859598853868194   |\n",
      "+------+---------------------+---------------------+---------------------+----------------------+\n",
      "alpha = 100 Starts CPD:\n",
      " +------+---------------------+---------------------+---------------------+----------------------+\n",
      "| F    | F(0)                | F(0)                | F(1)                | F(1)                 |\n",
      "+------+---------------------+---------------------+---------------------+----------------------+\n",
      "| T    | T(0)                | T(1)                | T(0)                | T(1)                 |\n",
      "+------+---------------------+---------------------+---------------------+----------------------+\n",
      "| S(0) | 0.5535714285714286  | 0.8678571428571429  | 0.6621621621621622  | 0.026256983240223464 |\n",
      "+------+---------------------+---------------------+---------------------+----------------------+\n",
      "| S(1) | 0.44642857142857145 | 0.13214285714285715 | 0.33783783783783783 | 0.9737430167597766   |\n",
      "+------+---------------------+---------------------+---------------------+----------------------+\n",
      "alpha = 1000 Starts CPD:\n",
      " +------+---------------------+--------------------+--------------------+---------------------+\n",
      "| F    | F(0)                | F(0)               | F(1)               | F(1)                |\n",
      "+------+---------------------+--------------------+--------------------+---------------------+\n",
      "| T    | T(0)                | T(1)               | T(0)               | T(1)                |\n",
      "+------+---------------------+--------------------+--------------------+---------------------+\n",
      "| S(0) | 0.5059288537549407  | 0.6410958904109589 | 0.5229007633587787 | 0.12142857142857143 |\n",
      "+------+---------------------+--------------------+--------------------+---------------------+\n",
      "| S(1) | 0.49407114624505927 | 0.3589041095890411 | 0.4770992366412214 | 0.8785714285714286  |\n",
      "+------+---------------------+--------------------+--------------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "bd_cpds1 = bayesEst.get_parameters(prior_type='BDeu',equivalent_sample_size=1)\n",
    "bd_cpds10 = bayesEst.get_parameters(prior_type='BDeu',equivalent_sample_size=10)\n",
    "bd_cpds100 = bayesEst.get_parameters(prior_type='BDeu',equivalent_sample_size=100)\n",
    "bd_cpds1000 = bayesEst.get_parameters(prior_type='BDeu',equivalent_sample_size=1000)\n",
    "\n",
    "print('True Starts CPD:\\n',cpdS)\n",
    "print('alpha = 1 Starts CPD:\\n',bd_cpds1[4])\n",
    "print('alpha = 10 Starts CPD:\\n',bd_cpds10[4])\n",
    "print('alpha = 100 Starts CPD:\\n',bd_cpds100[4])\n",
    "print('alpha = 1000 Starts CPD:\\n',bd_cpds1000[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As $\\alpha$ is increased, the estimated CPD table entries become more and more conservative.\n",
    "\n",
    "Additionally, performance is also dependent on the amount of data available, just as with MLE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Starts CPD:\n",
      " +------+------+------+------+------+\n",
      "| F    | F(0) | F(0) | F(1) | F(1) |\n",
      "+------+------+------+------+------+\n",
      "| T    | T(0) | T(1) | T(0) | T(1) |\n",
      "+------+------+------+------+------+\n",
      "| S(0) | 0.99 | 0.92 | 1.0  | 0.01 |\n",
      "+------+------+------+------+------+\n",
      "| S(1) | 0.01 | 0.08 | 0.0  | 0.99 |\n",
      "+------+------+------+------+------+\n",
      "10 Samples Starts CPD:\n",
      " +------+---------------------+--------------------+\n",
      "| F    | F(1)                | F(1)               |\n",
      "+------+---------------------+--------------------+\n",
      "| T    | T(0)                | T(1)               |\n",
      "+------+---------------------+--------------------+\n",
      "| S(0) | 0.6428571428571429  | 0.1956521739130435 |\n",
      "+------+---------------------+--------------------+\n",
      "| S(1) | 0.35714285714285715 | 0.8043478260869565 |\n",
      "+------+---------------------+--------------------+\n",
      "100 Samples Starts CPD:\n",
      " +------+--------------------+----------------------+\n",
      "| F    | F(0)               | F(1)                 |\n",
      "+------+--------------------+----------------------+\n",
      "| T    | T(1)               | T(1)                 |\n",
      "+------+--------------------+----------------------+\n",
      "| S(0) | 0.7352941176470589 | 0.023316062176165803 |\n",
      "+------+--------------------+----------------------+\n",
      "| S(1) | 0.2647058823529412 | 0.9766839378238342   |\n",
      "+------+--------------------+----------------------+\n",
      "1000 Samples Starts CPD:\n",
      " +------+---------------------+---------------------+---------------------+----------------------+\n",
      "| F    | F(0)                | F(0)                | F(1)                | F(1)                 |\n",
      "+------+---------------------+---------------------+---------------------+----------------------+\n",
      "| T    | T(0)                | T(1)                | T(0)                | T(1)                 |\n",
      "+------+---------------------+---------------------+---------------------+----------------------+\n",
      "| S(0) | 0.8529411764705882  | 0.943010752688172   | 0.9528301886792453  | 0.013342898134863702 |\n",
      "+------+---------------------+---------------------+---------------------+----------------------+\n",
      "| S(1) | 0.14705882352941177 | 0.05698924731182796 | 0.04716981132075472 | 0.9866571018651363   |\n",
      "+------+---------------------+---------------------+---------------------+----------------------+\n"
     ]
    }
   ],
   "source": [
    "bayesEst10 = BayesianEstimator(model, sample10)\n",
    "bayesEst100 = BayesianEstimator(model, sample100)\n",
    "bayesEst1000 = BayesianEstimator(model, sample1000)\n",
    "\n",
    "bd_cpds10 = bayesEst10.get_parameters(prior_type='BDeu',equivalent_sample_size=5)\n",
    "bd_cpds100 = bayesEst100.get_parameters(prior_type='BDeu',equivalent_sample_size=5)\n",
    "bd_cpds1000 = bayesEst1000.get_parameters(prior_type='BDeu',equivalent_sample_size=5)\n",
    "\n",
    "print('True Starts CPD:\\n',cpdS)\n",
    "print('10 Samples Starts CPD:\\n',bd_cpds10[4])\n",
    "print('100 Samples Starts CPD:\\n',bd_cpds100[4])\n",
    "print('1000 Samples Starts CPD:\\n',bd_cpds1000[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure Learning\n",
    "\n",
    "Now that we are able to fit conditional probabilities to a given bayesian-network structure, our next task is to generate a structure (Directed-Acyclic Graph) which best matches this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Scoring\n",
    "\n",
    "The first family of methods we will explore which achieve this are *network scoring* techniques. These methods are similar to traditional numerical optimization, where we are trying to find the model structure $M$ which maximizes the network score $p(M|D)$, which is the probability that our model fits the original data. This requires generating multiple possible models, and comparing their individual scores. In order to evaluate the score of a given model, we can compute an approximation of $p(M|D)$ given that\n",
    "\n",
    "$$p(M|D) \\propto p(D|M)p(M).$$\n",
    "\n",
    "From here we have to fit each model to the data, using one of the parameter learning methods described above. We will specifically use the bayesian parameter estimation method, since maximum likelihood estimation will tend towards more complex structures. Since we will be using a bayesian framework with Dirichlet priors, we can use the score equation we previously defined in the bayesian estimation section, along with our K2 or BDeu prior. Since the best performance was found using BDeu before, we will continue to use the BDeu prior in this section.\n",
    "\n",
    "### Exhaustive Search\n",
    "\n",
    "The simplest method to search through model space is an exhaustive search, i.e. testing every possible combination of edges and directions. While this is simple to implement and guarantees finding the structure which absolutely the best score, this method can quickly become computationally intractable. Given that a network with $n$ nodes has $\\frac{n(n-1)}{2}$ distinct pairs of nodes, then for every pair there are three types of connections, meaning that there are a total of $3^{\\frac{n(n-1)}{2}}$ possible connected graphs. This means that this approach is infeasable for all but the simplest of toy problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('B', 'T'), ('F', 'G'), ('F', 'S'), ('T', 'S')]\n",
      "Elapsed Time: 220.1201663017273  sec\n"
     ]
    }
   ],
   "source": [
    "es = ExhaustiveSearch(sample10000, scoring_method=BDeuScore(sample10000, equivalent_sample_size=1))\n",
    "start = time.time()\n",
    "es_model = es.estimate()\n",
    "end = time.time()\n",
    "print(es_model.edges())\n",
    "print(\"Elapsed Time:\", end-start, ' sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While simple, the exhaustive search is very slow. This method (along with other search methods) is also heavily tied to the samples used. As can be seen above, even for this simple network and 10,000 samples, the algorithm is only able to find four out of the five connections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hill Climbing\n",
    "\n",
    "Another, more practical search method is a *Hill Climbing* search. In this method, we can start from a network with no connections and iteratively generate single connections which maximize the network score. This is possible because the overall score is comprised of additive terms of each node family. Due to this relationship, the change in score due to a single edge is not influenced by any other edge and we can use local heuristics entirely. Since we are only using local heuristics, we stop searching when we reach a local maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Samples:  [('B', 'T'), ('B', 'S'), ('S', 'T')]\n",
      "Elapsed Time: 0.4325718879699707  sec\n",
      "100 Samples:  [('G', 'F'), ('F', 'S')]\n",
      "1000 Samples:  [('G', 'F'), ('F', 'S'), ('T', 'S'), ('T', 'B')]\n",
      "10000 Samples:  [('G', 'F'), ('F', 'S'), ('T', 'S'), ('T', 'B')]\n"
     ]
    }
   ],
   "source": [
    "hc = HillClimbSearch(sample10, scoring_method=BDeuScore(sample10, equivalent_sample_size=1))\n",
    "start = time.time()\n",
    "best_model = hc.estimate()\n",
    "end = time.time()\n",
    "print(\"10 Samples: \",best_model.edges())\n",
    "print(\"Elapsed Time:\", end-start, ' sec')\n",
    "\n",
    "hc = HillClimbSearch(sample100, scoring_method=BDeuScore(sample100, equivalent_sample_size=1))\n",
    "best_model = hc.estimate()\n",
    "print(\"100 Samples: \",best_model.edges())\n",
    "\n",
    "hc = HillClimbSearch(sample1000, scoring_method=BDeuScore(sample1000, equivalent_sample_size=1))\n",
    "best_model = hc.estimate()\n",
    "print(\"1000 Samples: \",best_model.edges())\n",
    "\n",
    "hc = HillClimbSearch(sample10000, scoring_method=BDeuScore(sample10000, equivalent_sample_size=1))\n",
    "best_model = hc.estimate()\n",
    "print(\"10000 Samples: \",best_model.edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this algorithm runs significantly faster than the exhaustive search method. However just like all of the other methods we have seen, its performance is dependent on the amount of data available. The outcome of the algorithm is also dependent on the value of $\\alpha$, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha = 1:  [('G', 'F'), ('F', 'S'), ('T', 'S'), ('T', 'B')]\n",
      "alpha = 10:  [('B', 'T'), ('G', 'B'), ('F', 'G'), ('F', 'S'), ('T', 'S')]\n",
      "alpha = 100:  [('B', 'G'), ('B', 'F'), ('F', 'G'), ('T', 'B'), ('T', 'F'), ('T', 'G'), ('S', 'F'), ('S', 'T'), ('S', 'B'), ('S', 'G')]\n",
      "alpha = 1000:  [('B', 'F'), ('B', 'G'), ('B', 'S'), ('G', 'F'), ('G', 'S'), ('F', 'S'), ('T', 'S'), ('T', 'B'), ('T', 'F'), ('T', 'G')]\n"
     ]
    }
   ],
   "source": [
    "hc = HillClimbSearch(sample10000, scoring_method=BDeuScore(sample10000, equivalent_sample_size=1))\n",
    "best_modelhc1 = hc.estimate()\n",
    "print(\"alpha = 1: \",best_modelhc1.edges())\n",
    "\n",
    "hc = HillClimbSearch(sample10000, scoring_method=BDeuScore(sample10000, equivalent_sample_size=10))\n",
    "best_modelhc10 = hc.estimate()\n",
    "print(\"alpha = 10: \",best_modelhc10.edges())\n",
    "\n",
    "hc = HillClimbSearch(sample10000, scoring_method=BDeuScore(sample10000, equivalent_sample_size=100))\n",
    "best_modelhc100 = hc.estimate()\n",
    "print(\"alpha = 100: \",best_modelhc100.edges())\n",
    "\n",
    "hc = HillClimbSearch(sample10000, scoring_method=BDeuScore(sample10000, equivalent_sample_size=1000))\n",
    "best_modelhc1000 = hc.estimate()\n",
    "print(\"alpha = 1000: \",best_modelhc1000.edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the value of $\\alpha$ is increased, the complexity of the optimized structure also increases. In this example, an $\\alpha$ of 10 gets the best performance, beating the exhaustive search method. However, while all of the correct links are present, the causality of one of the relationships is reversed.\n",
    "\n",
    "## PC Algorithm\n",
    "\n",
    "A different form of structure learning, which is different from those we have described above, is the *PC* algorithm. This is a constraint-based estimator, where a structure is created based on empirical local dependencies.\n",
    "\n",
    "### Determining Independence\n",
    "\n",
    "Since this algorithm is based on dependencies, a method is needed for testing variable independence. This is estimating using the distribution $p(x,y,z)$ over the variables x, y, and z by counting their occurances in the data. We compute the distribution of their conditional mutual information,\n",
    "\n",
    "$$2\\textit{N}MI(x;y|z),$$\n",
    "\n",
    "which is a $\\chi^2$ distribution with\n",
    "\n",
    "$$(dim(x)-1)(dim(y)-1)dim(z)$$\n",
    "\n",
    "degrees of freedom. If a sample is in the tails of the distribution within a significance threshold, then the variables are conditionally dependent.\n",
    "\n",
    "### Creating the Skeleton\n",
    "\n",
    "The PC algorithm first creates a skeleton of the bayesian network, over multiple rounds. The first round starts with an all-to-all connected, undirected graph, and the overall goal is to remove as many links as possible. In the first round, all $x-y$ pairs are tested for $x\\perp\\!\\!\\!\\perp y|\\emptyset$. If the variables are independent, then the link between them is removed.\n",
    "\n",
    "In the remaining rounds, all $x-y$ pairs are tested along with neighbors z, where the number of neighbors is increased by one every round. If $x\\perp\\!\\!\\!\\perp y|z$ is true, then the link $x-y$ is removed. This process is repeated until there is no change between rounds.\n",
    "\n",
    "### Orienting the Skeleton\n",
    "\n",
    "Once the skeleton has been created, we don't have a full bayesian belief network yet. In order for that, we need to orient the edges into a directed acyclic graph. For each set of neighboring variables $x-z-y$, the following test is performed. If $x\\perp\\!\\!\\!\\perp y|\\emptyset$ then z is a collider variable, and the links should be oriented as\n",
    "\n",
    "$$x\\rightarrow z \\leftarrow y.$$\n",
    "\n",
    "If $x\\perp\\!\\!\\!\\perp y|z$ is true however, then z is not a collider and the links are\n",
    "\n",
    "$$x\\leftarrow z \\rightarrow y.$$\n",
    "\n",
    "We can now run our algorithm for our example problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time:  0.315032958984375  sec\n",
      "Undirected edges:  [('G', 'F')]\n",
      "PDAG edges:        [('G', 'F'), ('F', 'G')]\n",
      "DAG edges:         [('B', 'G'), ('B', 'T'), ('F', 'G'), ('F', 'S'), ('T', 'S')]\n"
     ]
    }
   ],
   "source": [
    "PCest = ConstraintBasedEstimator(sample10)\n",
    "start = time.time()\n",
    "skel, seperating_sets = PCest.estimate_skeleton(significance_level=0.01)\n",
    "pdag = PCest.skeleton_to_pdag(skel, seperating_sets)\n",
    "PCmodel = PCest.pdag_to_dag(pdag)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Elapsed Time: \", end-start, \" sec\")\n",
    "print(\"Undirected edges: \", skel.edges())\n",
    "print(\"PDAG edges:       \", pdag.edges())\n",
    "print(\"DAG edges:        \", model.edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, for our example problem the PC algorithm is the best structure-learning algorithm we can use. All of our edges are corrrectly identified with the correct causality relationships, and can be done in a similar time with three orders-of-magnitude less samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
