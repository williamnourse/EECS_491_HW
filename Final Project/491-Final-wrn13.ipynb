{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# EECS 491 Final Project: Bayesian Structure and Parameter Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "Bayesian-network models are a commonly-used framework for modeling conditional probabilities, allowing extremely complicated conditional relationships to be modeled and implemented with ease. Previously in this course, we have covered in great detail how to perform inference operations given a defined bayesian-network model. This works well when we are able to construct an informed model, which includes the correct conditional probabilities and links. However, what are we to do when we are given lots of data, with no knowledge of the relationships between any of the variables or elements? Before we can perform any kind of predictive inference, we first need to learn a bayesian model from our data. This can generally be described in two parts: learning the structural relationship between probabilistic variables, and learning the conditional probability distributions which fit data to a given structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Graph, Digraph\n",
    "import pgmpy\n",
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.factors.discrete import TabularCPD\n",
    "from pgmpy.inference import VariableElimination\n",
    "from pgmpy.sampling import GibbsSampling\n",
    "from pgmpy.estimators import ExhaustiveSearch, HillClimbSearch, ConstraintBasedEstimator\n",
    "from pgmpy.estimators import BDeuScore, K2Score, BicScore\n",
    "from pgmpy.estimators import MaximumLikelihoodEstimator, BayesianEstimator\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## True Model\n",
    "\n",
    "In order to generate bayesian-networks from scratch, we need data to describe. Throughout this example, we will be using the following model (Adpted from Barber Exercise 3.6) that describes the probability of a car starting given certain factors. We will use this model to generate samples which will be used to generate networks and parameters, and then we will compare the accuracy of the generated models to the actual model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"220pt\" height=\"188pt\"\n",
       " viewBox=\"0.00 0.00 220.35 188.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 184)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-184 216.3454,-184 216.3454,4 -4,4\"/>\n",
       "<!-- b -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>b</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"93.3454\" cy=\"-162\" rx=\"36.2938\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"93.3454\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Battery</text>\n",
       "</g>\n",
       "<!-- g -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>g</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"154.3454\" cy=\"-90\" rx=\"33.5952\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"154.3454\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Gauge</text>\n",
       "</g>\n",
       "<!-- b&#45;&gt;g -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>b&#45;&gt;g</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M107.4934,-145.3008C115.2694,-136.1226 125.0586,-124.568 133.6757,-114.3971\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"136.4805,-116.501 140.2742,-106.6087 131.1396,-111.9761 136.4805,-116.501\"/>\n",
       "</g>\n",
       "<!-- t -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>t</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"51.3454\" cy=\"-90\" rx=\"51.1914\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"51.3454\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Turns Over</text>\n",
       "</g>\n",
       "<!-- b&#45;&gt;t -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>b&#45;&gt;t</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M83.1784,-144.5708C78.2987,-136.2055 72.3443,-125.998 66.911,-116.6839\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"69.7726,-114.6431 61.7106,-107.7689 63.7262,-118.1702 69.7726,-114.6431\"/>\n",
       "</g>\n",
       "<!-- f -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>f</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"185.3454\" cy=\"-162\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"185.3454\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Fuel</text>\n",
       "</g>\n",
       "<!-- f&#45;&gt;g -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>f&#45;&gt;g</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M177.8412,-144.5708C174.277,-136.2927 169.936,-126.2104 165.9598,-116.9752\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"169.1653,-115.5696 161.9959,-107.7689 162.7359,-118.3379 169.1653,-115.5696\"/>\n",
       "</g>\n",
       "<!-- s -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>s</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"133.3454\" cy=\"-18\" rx=\"30.5947\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"133.3454\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Starts</text>\n",
       "</g>\n",
       "<!-- f&#45;&gt;s -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>f&#45;&gt;s</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M193.1016,-144.5674C200.2504,-125.7868 208.239,-95.4445 197.3454,-72 190.4767,-57.2174 177.3474,-45.025 164.7594,-35.9896\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"166.4665,-32.9199 156.2147,-30.2457 162.5613,-38.7293 166.4665,-32.9199\"/>\n",
       "</g>\n",
       "<!-- t&#45;&gt;s -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>t&#45;&gt;s</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M70.7783,-72.937C82.1226,-62.9762 96.5464,-50.3114 108.6854,-39.6527\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"111.0721,-42.2149 116.2772,-32.9868 106.4534,-36.9548 111.0721,-42.2149\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f34dc0161d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q5 = Digraph()\n",
    "q5.node('b', 'Battery')\n",
    "q5.node('g', 'Gauge')\n",
    "q5.node('f', 'Fuel')\n",
    "q5.node('t', 'Turns Over')\n",
    "q5.node('s', 'Starts')\n",
    "q5.edges(['bg','fg','bt','ts','fs'])\n",
    "q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is valid:  True\n",
      "True Model Edges:  [('B', 'G'), ('B', 'T'), ('F', 'G'), ('F', 'S'), ('T', 'S')]\n"
     ]
    }
   ],
   "source": [
    "# Create model from list of edges\n",
    "model = BayesianModel([('B','G'),('F','G'),('B','T'),('T','S'),('F','S')])\n",
    "\n",
    "# define p(B) and p(M) \n",
    "# variable_card is cardinality = 2 for true|false\n",
    "# values are defined in numeric order p(x_i = [false, true]), ie  [0, 1]\n",
    "priorB = TabularCPD(variable='B', variable_card=2, values=[[0.02, 0.98]])\n",
    "priorF = TabularCPD(variable='F', variable_card=2, values=[[0.05, 0.95]])\n",
    "\n",
    "# define p(G|B,F)\n",
    "# Variables cycle in numerical order of evidence values,\n",
    "# ie BF = 00, 01, 10, 11 for each value of G.\n",
    "cpdG = TabularCPD(variable='G', variable_card=2, \n",
    "                  evidence=['B', 'F'], evidence_card=[2, 2],\n",
    "                  values=[[0.99, 0.1, 0.97, 0.04], \n",
    "                          [0.01, 0.9, 0.03, 0.96]])\n",
    "\n",
    "# define p(T|B)\n",
    "cpdT = TabularCPD(variable='T', variable_card=2,\n",
    "                 evidence=['B'], evidence_card=[2],\n",
    "                 values=[[0.98, 0.03],\n",
    "                         [0.02, 0.97]])\n",
    "\n",
    "# define p(S|T,F)\n",
    "cpdS = TabularCPD(variable='S', variable_card=2, \n",
    "                  evidence=['F','T'], evidence_card=[2, 2],\n",
    "                  values=[[0.99, 0.92, 1.0, 0.01], \n",
    "                          [0.01, 0.08, 0.0, 0.99]])\n",
    "\n",
    "# add probabilities to model\n",
    "model.add_cpds(priorB, priorF, cpdG, cpdT, cpdS)\n",
    "print('Model is valid: ', model.check_model())\n",
    "print('True Model Edges: ',model.edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a defined model, let's generate some data using Gibbs sampling. Different amounts of samples are collected, in order to test the number of required samples for each of the later-implemented algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 1132.47it/s]\n",
      "100%|██████████| 99/99 [00:00<00:00, 1471.87it/s]\n",
      "100%|██████████| 999/999 [00:00<00:00, 1577.62it/s]\n",
      "100%|██████████| 9999/9999 [00:04<00:00, 2380.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# generate sets of samples\n",
    "sampler = GibbsSampling(model)\n",
    "sample10 = sampler.sample(size=10, return_type='dataframe')\n",
    "sample100 = sampler.sample(size=100, return_type='dataframe')\n",
    "sample1000 = sampler.sample(size=1000, return_type='dataframe')\n",
    "sample10000 = sampler.sample(size=10000, return_type='dataframe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Learning\n",
    "\n",
    "In this section, we will focus on the problem of fitting a given bayesian structure to real data. Specifically, this requires finding the conditional probability distributions (CPDs) for each variable such that the joint probability distribution $p(x)$ accurately describes the observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimation\n",
    "\n",
    "We can assume that in a bayesian network, the structure of $p(x)$ can be constrained as\n",
    "\n",
    "$$p(x) = \\prod_{i=1}^{K}p(x_i|pa(x_i)),$$\n",
    "\n",
    "where $K$ is the number of variables, $x_i$ is a given variable and $pa(x_i)$ are the parents of $x_i$ in the directed acyclic graph.\n",
    "\n",
    "One method of extimating the CPDs for each variable is to determine the maximum likelihood of each term. To do this, we can minimize the KL divergence between the data distribution $q(x)$ and $p(x)$. The setting which minimizes this divergence and maximizes the overall likelihood is\n",
    "\n",
    "$$p(x_i|pa(x_i)) = q(x_i|pa(x_i),$$\n",
    "\n",
    "which can be written in terms of the given data,\n",
    "\n",
    "$$p(x_i=s|pa(x_i)=t) \\propto \\sum_{n=1}^{N}\\mathbb{I}[x_i^n=s,pa(x_i^n)=t],$$\n",
    "\n",
    "saying that each CPD table entry can be set by using the relative frequencies of each state, meaning that each entry is the ratio of the number of times the state $\\{x_i=s,pa(x_i)=t\\}$ occurs vs the sum of all other states $\\{x_i=s',pa(x_i)=t\\}$. This operation is fairly computationally efficient, as it just requires counting.\n",
    "\n",
    "Below, we fit CPDs to our example problem given the correct structure, and compare our results to the true values using 10,000 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Starts CPD:\n",
      " +------+------+------+------+------+\n",
      "| F    | F(0) | F(0) | F(1) | F(1) |\n",
      "+------+------+------+------+------+\n",
      "| T    | T(0) | T(1) | T(0) | T(1) |\n",
      "+------+------+------+------+------+\n",
      "| S(0) | 0.99 | 0.92 | 1.0  | 0.01 |\n",
      "+------+------+------+------+------+\n",
      "| S(1) | 0.01 | 0.08 | 0.0  | 0.99 |\n",
      "+------+------+------+------+------+\n",
      "MLE Starts CPD:\n",
      " +------+------+---------------------+------+----------------------+\n",
      "| F    | F(0) | F(0)                | F(1) | F(1)                 |\n",
      "+------+------+---------------------+------+----------------------+\n",
      "| T    | T(0) | T(1)                | T(0) | T(1)                 |\n",
      "+------+------+---------------------+------+----------------------+\n",
      "| S(0) | 1.0  | 0.9113924050632911  | 1.0  | 0.008637655805816751 |\n",
      "+------+------+---------------------+------+----------------------+\n",
      "| S(1) | 0.0  | 0.08860759493670886 | 0.0  | 0.9913623441941832   |\n",
      "+------+------+---------------------+------+----------------------+\n"
     ]
    }
   ],
   "source": [
    "mle_model = MaximumLikelihoodEstimator(model, sample10000)\n",
    "mle_cpds = mle_model.get_parameters()\n",
    "\n",
    "print('True Starts CPD:\\n',cpdS)\n",
    "print('MLE Starts CPD:\\n',mle_cpds[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One CPD is displayed, for the sake of brevity. As we can see, the MLE method is able to approximate each of the tabld entries within an error of about 1%. Note however that the relationship in the first column is lost, as following the MLE results the car should never start in that condition. While that is acceptable in this problem, it may not be in others.\n",
    "\n",
    "As we can see below, the accuracy of our model fit is dependent on the number and diversity of samples we receive. As the amount of available data decreases, our error in each entry also increases, and the overall CPD has less possibilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Starts CPD:\n",
      " +------+------+------+------+------+\n",
      "| F    | F(0) | F(0) | F(1) | F(1) |\n",
      "+------+------+------+------+------+\n",
      "| T    | T(0) | T(1) | T(0) | T(1) |\n",
      "+------+------+------+------+------+\n",
      "| S(0) | 0.99 | 0.92 | 1.0  | 0.01 |\n",
      "+------+------+------+------+------+\n",
      "| S(1) | 0.01 | 0.08 | 0.0  | 0.99 |\n",
      "+------+------+------+------+------+\n",
      "1000 Samples:\n",
      " +------+------+---------------------+------+---------------------+\n",
      "| F    | F(0) | F(0)                | F(1) | F(1)                |\n",
      "+------+------+---------------------+------+---------------------+\n",
      "| T    | T(0) | T(1)                | T(0) | T(1)                |\n",
      "+------+------+---------------------+------+---------------------+\n",
      "| S(0) | 1.0  | 0.9478260869565217  | 1.0  | 0.01264367816091954 |\n",
      "+------+------+---------------------+------+---------------------+\n",
      "| S(1) | 0.0  | 0.05217391304347826 | 0.0  | 0.9873563218390805  |\n",
      "+------+------+---------------------+------+---------------------+\n",
      "100 Samples:\n",
      " +------+---------------------+----------------------+\n",
      "| F    | F(0)                | F(1)                 |\n",
      "+------+---------------------+----------------------+\n",
      "| T    | T(1)                | T(1)                 |\n",
      "+------+---------------------+----------------------+\n",
      "| S(0) | 0.8333333333333334  | 0.010638297872340425 |\n",
      "+------+---------------------+----------------------+\n",
      "| S(1) | 0.16666666666666666 | 0.9893617021276596   |\n",
      "+------+---------------------+----------------------+\n",
      "10 Samples:\n",
      " +------+------+--------------------+\n",
      "| F    | F(1) | F(1)               |\n",
      "+------+------+--------------------+\n",
      "| T    | T(0) | T(1)               |\n",
      "+------+------+--------------------+\n",
      "| S(0) | 1.0  | 0.1111111111111111 |\n",
      "+------+------+--------------------+\n",
      "| S(1) | 0.0  | 0.8888888888888888 |\n",
      "+------+------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "mle_model1000 = MaximumLikelihoodEstimator(model, sample1000)\n",
    "mle_cpds1000 = mle_model1000.get_parameters()\n",
    "mle_model100 = MaximumLikelihoodEstimator(model, sample100)\n",
    "mle_cpds100 = mle_model100.get_parameters()\n",
    "mle_model10 = MaximumLikelihoodEstimator(model, sample10)\n",
    "mle_cpds10 = mle_model10.get_parameters()\n",
    "\n",
    "print('True Starts CPD:\\n',cpdS)\n",
    "print('1000 Samples:\\n',mle_cpds1000[3])\n",
    "print('100 Samples:\\n',mle_cpds100[3])\n",
    "print('10 Samples:\\n',mle_cpds10[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Starts CPD:\n",
      " +------+------+------+------+------+\n",
      "| F    | F(0) | F(0) | F(1) | F(1) |\n",
      "+------+------+------+------+------+\n",
      "| T    | T(0) | T(1) | T(0) | T(1) |\n",
      "+------+------+------+------+------+\n",
      "| S(0) | 0.99 | 0.92 | 1.0  | 0.01 |\n",
      "+------+------+------+------+------+\n",
      "| S(1) | 0.01 | 0.08 | 0.0  | 0.99 |\n",
      "+------+------+------+------+------+\n",
      "K2 Starts CPD:\n",
      " +------+------+---------------------+---------------------+----------------------+\n",
      "| F    | F(0) | F(0)                | F(1)                | F(1)                 |\n",
      "+------+------+---------------------+---------------------+----------------------+\n",
      "| T    | T(0) | T(1)                | T(0)                | T(1)                 |\n",
      "+------+------+---------------------+---------------------+----------------------+\n",
      "| S(0) | 0.8  | 0.9401709401709402  | 0.9285714285714286  | 0.013761467889908258 |\n",
      "+------+------+---------------------+---------------------+----------------------+\n",
      "| S(1) | 0.2  | 0.05982905982905983 | 0.07142857142857142 | 0.9862385321100917   |\n",
      "+------+------+---------------------+---------------------+----------------------+\n",
      "BDeu Starts CPD:\n",
      " +------+---------------------+---------------------+---------------------+----------------------+\n",
      "| F    | F(0)                | F(0)                | F(1)                | F(1)                 |\n",
      "+------+---------------------+---------------------+---------------------+----------------------+\n",
      "| T    | T(0)                | T(1)                | T(0)                | T(1)                 |\n",
      "+------+---------------------+---------------------+---------------------+----------------------+\n",
      "| S(0) | 0.8529411764705882  | 0.943010752688172   | 0.9528301886792453  | 0.013342898134863702 |\n",
      "+------+---------------------+---------------------+---------------------+----------------------+\n",
      "| S(1) | 0.14705882352941177 | 0.05698924731182796 | 0.04716981132075472 | 0.9866571018651363   |\n",
      "+------+---------------------+---------------------+---------------------+----------------------+\n"
     ]
    }
   ],
   "source": [
    "bayesEst = BayesianEstimator(model, sample1000)\n",
    "k2_cpds = bayesEst.get_parameters(prior_type='K2')\n",
    "bd_cpds = bayesEst.get_parameters(prior_type='BDeu',equivalent_sample_size=5)\n",
    "\n",
    "print('True Starts CPD:\\n',cpdS)\n",
    "print('K2 Starts CPD:\\n',k2_cpds[4])\n",
    "print('BDeu Starts CPD:\\n',bd_cpds[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure Learning\n",
    "\n",
    "Now that we are able to fit conditional probabilities to a given bayesian-network structure, our next task is to generate a structure (Directed-Acyclic Graph) which best matches this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Scoring\n",
    "\n",
    "The first family of methods we will explore which achieve this are *network scoring* techniques. These methods are similar to traditional numerical optimization, where we are trying to find the model structure $M$ which maximizes the network score $p(M|D)$, which is the probability that our model fits the original data. This requires generating multiple possible models, and comparing their individual scores. In order to evaluate the score of a given model, we can compute an approximation of $p(M|D)$ given that\n",
    "\n",
    "$$p(M|D) \\propto p(D|M)p(M).$$\n",
    "\n",
    "From here we have to fit each model to the data, using one of the parameter learning methods described above. We will specifically use the bayesian parameter estimation method, since maximum likelihood estimation will tend towards more complex structures.\n",
    "\n",
    "### Exhaustive Search\n",
    "\n",
    "The simplest method to search through model space is an exhaustive search, i.e. testing every possible combination of edges and directions. While this is simple to implement and guarantees finding the structure which absolutely the best score, this method can quickly become computationally intractable. Given that a network with $n$ nodes has $\\frac{n(n-1)}{2}$ distinct pairs of nodes, then for every pair there are three types of connections, meaning that there are a total of $3^{\\frac{n(n-1)}{2}}$ possible connected graphs. This means that this approach is infeasable for all but the simplest of toy problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('B', 'T'), ('F', 'G'), ('F', 'S'), ('T', 'S')]\n",
      "Elapsed Time: 180.61312747001648  sec\n"
     ]
    }
   ],
   "source": [
    "es = ExhaustiveSearch(sample10000, scoring_method=K2Score(sample10000))\n",
    "start = time.time()\n",
    "es_model = es.estimate()\n",
    "end = time.time()\n",
    "print(es_model.edges())\n",
    "print(\"Elapsed Time:\", end-start, ' sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While simple, the exhaustive search is very slow. This method (along with other search methods) is also heavily tied to the samples used. As can be seen above, even for this simple network and 10,000 samples, the algorithm is only able to find four out of the five connections, and one of them has the incorrect independence relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hill Climbing\n",
    "\n",
    "Another, more practical search method is a *Hill Climbing* search. In this method, we can start from a network with no connections and iteratively generate single connections which maximize the network score. This is possible because the overall score is comprised of additive terms of each node family. Due to this relationship, the change in score due to a single edge is not influenced by any other edge and we can use local heuristics entirely. Since we are only using local heuristics, we stop searching when we reach a local maximum.\n",
    "\n",
    "#### Compare over \\# samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('B', 'S'), ('T', 'S')]\n",
      "Elapsed Time: 0.3474843502044678  sec\n"
     ]
    }
   ],
   "source": [
    "hc = HillClimbSearch(sample10, scoring_method=K2Score(sample10))\n",
    "start = time.time()\n",
    "best_model = hc.estimate()\n",
    "end = time.time()\n",
    "print(best_model.edges())\n",
    "print(\"Elapsed Time:\", end-start, ' sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('F', 'G'), ('S', 'F')]\n",
      "Elapsed Time: 0.3451557159423828  sec\n"
     ]
    }
   ],
   "source": [
    "hc = HillClimbSearch(sample100, scoring_method=K2Score(sample100))\n",
    "start = time.time()\n",
    "best_model = hc.estimate()\n",
    "end = time.time()\n",
    "print(best_model.edges())\n",
    "print(\"Elapsed Time:\", end-start, ' sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('B', 'T'), ('F', 'G'), ('F', 'S'), ('T', 'S')]\n",
      "Elapsed Time: 0.46822309494018555  sec\n"
     ]
    }
   ],
   "source": [
    "hc = HillClimbSearch(sample10000, scoring_method=K2Score(sample10000))\n",
    "start = time.time()\n",
    "best_modelhck2 = hc.estimate()\n",
    "end = time.time()\n",
    "print(best_modelhck2.edges())\n",
    "print(\"Elapsed Time:\", end-start, ' sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare across score method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('B', 'F'), ('B', 'S'), ('B', 'G'), ('G', 'F'), ('G', 'S'), ('F', 'S'), ('T', 'S'), ('T', 'B'), ('T', 'F'), ('T', 'G')]\n",
      "Elapsed Time: 0.8934545516967773  sec\n"
     ]
    }
   ],
   "source": [
    "hc = HillClimbSearch(sample10000, scoring_method=BDeuScore(sample10000, equivalent_sample_size=10000))\n",
    "start = time.time()\n",
    "best_modelhcBD = hc.estimate()\n",
    "end = time.time()\n",
    "print(best_modelhcBD.edges())\n",
    "print(\"Elapsed Time:\", end-start, ' sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PC Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time:  0.315032958984375  sec\n",
      "Undirected edges:  [('G', 'F')]\n",
      "PDAG edges:        [('G', 'F'), ('F', 'G')]\n",
      "DAG edges:         [('B', 'G'), ('B', 'T'), ('F', 'G'), ('F', 'S'), ('T', 'S')]\n"
     ]
    }
   ],
   "source": [
    "PCest = ConstraintBasedEstimator(sample10)\n",
    "start = time.time()\n",
    "skel, seperating_sets = PCest.estimate_skeleton(significance_level=0.01)\n",
    "pdag = PCest.skeleton_to_pdag(skel, seperating_sets)\n",
    "PCmodel = PCest.pdag_to_dag(pdag)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Elapsed Time: \", end-start, \" sec\")\n",
    "print(\"Undirected edges: \", skel.edges())\n",
    "print(\"PDAG edges:       \", pdag.edges())\n",
    "print(\"DAG edges:        \", model.edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"inferenceTrue = VariableElimination(model)\\nhck2 = BayesianModel(best_modelhck2.edges())\\nhck2.add_cpds(best_modelhck2.get_cpds())\\ninferenceHCK2 = VariableElimination(best_modelhck2)\\ninferenceHCBD = VariableElimination(best_modelhcBD.edges())\\ninferencePC = VariableElimination(PCmodel.edges())\\n\\nprint('p(f|s=0,t=0)')\\nprint('True:')\\nprint(inferenceTrue.query(['F'], evidence={'S': 0, 'T': 0}))\\nprint('Score (K2):')\\nprint(inferenceHCK2.query(['F'], evidence={'S': 0, 'T': 0}))\\nprint('Score (BDeau):')\\nprint(inferenceHCBD.query(['F'], evidence={'S': 0, 'T': 0}))\\nprint('PC:')\\nprint(inferencePC.query(['F'], evidence={'S': 0, 'T': 0}))\\n\\nprint('p(b|s=0,t=0)')\\nprint(inference.query(['B'], evidence={'S': 0, 'T': 0}))\\nprint('p(f|s=0,t=1)')\\nprint(inference.query(['F'], evidence={'S': 0, 'T': 1}))\\nprint('p(b|s=0,t=1)')\\nprint(inference.query(['B'], evidence={'S': 0, 'T': 1}))\""
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''inferenceTrue = VariableElimination(model)\n",
    "hck2 = BayesianModel(best_modelhck2.edges())\n",
    "hck2.add_cpds(best_modelhck2.get_cpds())\n",
    "inferenceHCK2 = VariableElimination(best_modelhck2)\n",
    "inferenceHCBD = VariableElimination(best_modelhcBD.edges())\n",
    "inferencePC = VariableElimination(PCmodel.edges())\n",
    "\n",
    "print('p(f|s=0,t=0)')\n",
    "print('True:')\n",
    "print(inferenceTrue.query(['F'], evidence={'S': 0, 'T': 0}))\n",
    "print('Score (K2):')\n",
    "print(inferenceHCK2.query(['F'], evidence={'S': 0, 'T': 0}))\n",
    "print('Score (BDeau):')\n",
    "print(inferenceHCBD.query(['F'], evidence={'S': 0, 'T': 0}))\n",
    "print('PC:')\n",
    "print(inferencePC.query(['F'], evidence={'S': 0, 'T': 0}))\n",
    "\n",
    "print('p(b|s=0,t=0)')\n",
    "print(inference.query(['B'], evidence={'S': 0, 'T': 0}))\n",
    "print('p(f|s=0,t=1)')\n",
    "print(inference.query(['F'], evidence={'S': 0, 'T': 1}))\n",
    "print('p(b|s=0,t=1)')\n",
    "print(inference.query(['B'], evidence={'S': 0, 'T': 1}))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
